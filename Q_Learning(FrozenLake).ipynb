{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEdXFFJrUHpa",
        "outputId": "0d28cf17-937a-486f-ef21-7aaf5741f370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.4.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import random"
      ],
      "metadata": {
        "id": "6qPHEbpMUTIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env=gym.make(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "id": "RcBSLoOEUYiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size=env.action_space.n # columns\n",
        "state_space_size=env.observation_space.n # rows\n",
        "\n",
        "q_table=np.zeros((state_space_size,action_space_size))\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr51EiuUUapD",
        "outputId": "cc29dc4a-84b1-490d-8fdc-40821e661da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes=10000 # total number of episodes to play during training\n",
        "max_steps_per_episode=100 # maximum step to take within single episode\n",
        "\n",
        "learning_rate=0.1\n",
        "discount_rate=0.99\n",
        "\n",
        "exploration_rate=1\n",
        "max_exploration_rate=1\n",
        "min_exploration_rate=0.01\n",
        "exploration_decay_rate=0.001"
      ],
      "metadata": {
        "id": "RnEloy3dUmv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes=[]\n",
        "## Q-learning algorithm\n",
        "# what happen in single episode:\n",
        "for episode in range(num_episodes):\n",
        "    state=env.reset() # reset for each new episode\n",
        "    done=False # keep track whether or not the episode finish, start=False\n",
        "    rewards_current_episode=0 # track reward, start=0\n",
        "    # what happen in each step of episode:\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Exploration & exploitation trade off\n",
        "        exploration_rate_threshold=random.uniform(0,1) # create random number (r) at each time step\n",
        "        if exploration_rate_threshold > exploration_rate:\n",
        "            action=np.argmax(q_table[state,:])\n",
        "        else:\n",
        "            action=env.action_space.sample()\n",
        "            \n",
        "        new_state,reward,done,info=env.step(action)# apply action & get results\n",
        "        \n",
        "        #Update Q-table:\n",
        "        q_table[state,action]=q_table[state,action]*(1-learning_rate) + \\\n",
        "        learning_rate*(reward+discount_rate*np.max(q_table[new_state,:]))\n",
        "        \n",
        "        state=new_state\n",
        "        rewards_current_episode+=reward\n",
        "        \n",
        "        if done==True:\n",
        "            break\n",
        "    # Exploration rate decay\n",
        "    exploration_rate=min_exploration_rate+\\\n",
        "    (max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
        "    \n",
        "    rewards_all_episodes.append(rewards_current_episode)"
      ],
      "metadata": {
        "id": "q4qO4ev8UqH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print average reward per thousand episodes\n",
        "rewards_per_thousand_episodes=np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count=1000\n",
        "print(\"Average reward per thousand episodes\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count,':',str(sum(r/1000)))\n",
        "    count+=1000\n",
        "\n",
        "#Print update Q table\n",
        "print('\\n\\nQ-table\\n')\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2vdn39jU0uD",
        "outputId": "9554e02f-a446-43c9-9f37-73c57c30d56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward per thousand episodes\n",
            "\n",
            "1000 : 0.04300000000000003\n",
            "2000 : 0.22300000000000017\n",
            "3000 : 0.4100000000000003\n",
            "4000 : 0.5520000000000004\n",
            "5000 : 0.6420000000000005\n",
            "6000 : 0.6900000000000005\n",
            "7000 : 0.6660000000000005\n",
            "8000 : 0.7110000000000005\n",
            "9000 : 0.6600000000000005\n",
            "10000 : 0.6800000000000005\n",
            "\n",
            "\n",
            "Q-table\n",
            "\n",
            "[[0.52014459 0.48141041 0.47639641 0.48190952]\n",
            " [0.24573569 0.28467066 0.32497345 0.47021687]\n",
            " [0.42582832 0.41799867 0.40257151 0.4398834 ]\n",
            " [0.25148924 0.18915962 0.33943434 0.42777979]\n",
            " [0.54097572 0.2458842  0.34278903 0.36862783]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.15981744 0.16653198 0.2739735  0.15429286]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.43884179 0.26834363 0.31398568 0.56464713]\n",
            " [0.44145192 0.61916438 0.4031545  0.41525321]\n",
            " [0.50670688 0.42389761 0.36826324 0.33044433]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.47518972 0.60851627 0.75593838 0.51700725]\n",
            " [0.77146825 0.90287045 0.78874018 0.77902415]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      \n",
        "# see the agent actual play\n",
        "for episode in range(3):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  print(\"Episode\",episode+1,\"\\n\")\n",
        "  time.sleep(1) \n",
        "\n",
        "  for step in range (max_steps_per_episode):\n",
        "    clear_output(wait=True)# clear output for another output printout,for visual smoothness\n",
        "    print(env.render(mode=\"ansi\")) #see where agent on the grid\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    action=np.argmax(q_table[state,:])\n",
        "    new_state,reward,done,info=env.step(action)\n",
        "\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      print(env.render(mode=\"ansi\"))\n",
        "      if reward==1:\n",
        "        print('Win')\n",
        "        time.sleep(3)\n",
        "      else:\n",
        "        print('Fell hole')\n",
        "        time.sleep(3)\n",
        "      clear_output(wait=True)\n",
        "      break\n",
        "\n",
        "    state=new_state # if  the last action do not complete the episode, then skip the conditional, move to new state &  next time step\n",
        "env.close() # close the environment\n",
        "      "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFPePEYUU6xH",
        "outputId": "d6f2be34-360a-48fe-c7dd-083ec3d0da68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Win\n"
          ]
        }
      ]
    }
  ]
}